---
title: "p8105_hw5_ar4459_practice"
author: "ASHLEY ROMO"
date: "2023-11-04"
output: github_document
---
```{r}
library(tidyverse)
library(purrr)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```
 
## Problem 1
For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 



## Problem 2
```{r}
# loading data   
data = 
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)) |> 
    mutate(data = map(path, read_csv)) |> 
    unnest() 

# tidying data
data_clean = 
  data |> 
  mutate(
  subject_ID = str_extract(files, "\\d+"),
  tx_arm = str_extract(files, "con|exp")) |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to =  "value") |> 
  select(subject_ID, tx_arm, week, value) |>
  mutate(week = as.numeric(week))

min(pull(data_clean |> filter(tx_arm == "con")))
```

Spaghetti plot
```{r}
plot = 
  data_clean |> 
  ggplot(aes(x = week, y = value, color = subject_ID)) +
  geom_line() +
  facet_wrap(~tx_arm) +
  labs(
    x = "Week",
    y = "Values",
    color = "Subject ID",
    title = "Observations For Each Subject Over Time"
  )

plot
```
The spaghetti plot shows that the control group has lower values than the experimental group. The control group appreas to stay between -2.5 and 5 while the experimental group increases over time. The mean value for the control group is `r mean(pull(data_clean |> filter(tx_arm == "con")))` and the mean value for the experimental group is `r mean(pull(data_clean |> filter(tx_arm == "exp")))`. The minimum value for the control group is `r min(pull(data_clean |> filter(tx_arm == "con")))` and the miniumum value for the treatment group is `r min(pull(data_clean |> filter(tx_arm == "exp")))`. The maximum value for the control group is `r max(pull(data_clean |> filter(tx_arm == "con")))` and the maximum value for the experimental group is `r max(pull(data_clean |> filter(tx_arm == "exp")))`.

## Problem 3

```{r}
# generate data
t_test_sim = function(mu = 0) {
  
  data = 
    tibble(
      x = rnorm(n = 30, mean = mu, sd = 5)
    )
  
  t_test = 
    t.test(data, mean = 0, conf.level = 0.95) |> 
    broom::tidy()
}
```

```{r}
# Generate 5000 datasets from the model

output = vector("list", 5000)

for (i in 5000) {
  output[[i]] = t_test_sim(30)
}

sim_results = bind_rows(output)
sim_results
```

```{r}
# Repeat the above for μ={1,2,3,4,5,6}
mu = tibble(mu = c(1, 2, 3, 4, 5, 6))
N = 20

#how do I do this 5000 times???                       
data_mu = c()
for (i in 1:N) { #
  data_mu[[i]] =
    mu |> 
    mutate(output = map(mu, t_test_sim)) 
}

sim_results_mu = 
  bind_rows(data_mu) |> 
  unnest(output)
```


```{r}
# Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
sim_results_mu_power =
  sim_results_mu |> 
  mutate(
    reject = p.value < 0.05
  ) |> 
  group_by(mu) |> 
  summarize(
    power = sum(reject)/N
  )


power_mu_plot =
  sim_results_mu_power |> 
  ggplot(aes(x = power, y = mu)) +
  geom_point() +
  geom_line() +
  labs(
    x = "Power",
    y = "Value of Mu",
    title = "Power vs. Mu"
  )

power_mu_plot
```
The plot shows that as the value of mu (effect size) increases, the power also increases. There is a positive association between power and mu.

 
```{r}
# Make a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis.
avg_mu = 
  sim_results_mu |> 
  group_by(mu) |> 
  summarize(
    mean_mu_estimate = mean(estimate)
  )

avg_mu_plot = 
  avg_mu |> 
  ggplot(aes(x = mu, y = mean_mu_estimate)) +
  geom_point() + 
  geom_line() +
  labs(
    x = "True Value of Mu",
    y = "Average Estimate of Mu",
    title = "True Value vs Average Estimate of Mu"
  )

avg_mu_plot
```
Based on the plot, the value of the average mu estimate closely approximates the true value of mu.


```{r}
# Make a second plot (or overlay on the first) the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ ? Why or why not?

avg_mu__power_reject = 
  sim_results_mu |> 
  mutate(
    reject = p.value < 0.05
  ) |> 
  group_by(mu) |> 
  filter(reject == "TRUE") |> 
  summarize(
    power = sum(reject)/N,
    mean_mu_estimate = mean(estimate)
  )

avg_mu__power_reject_plot = 
  avg_mu__power_reject |> 
  ggplot(aes(x = mu, y = mean_mu_estimate)) +
  geom_point() + 
  geom_line() +
  labs(
    x = "True Value of Mu",
    y = "Average Estimate of Mu",
    title = "True Value vs Average Estimate of Mu 
    (When Null Is Rejected)"
  )

avg_mu__power_reject_plot
```
According to the plot, the sample average of μ̂ across tests for which the null is rejected is approximately equal to the true value of μ. For example, when the true value of mu is 2, the average mu is 2.42. When the true value of 3, the average mu is 3.27. When the true value of mu is 4, the average mu is 4.23. When the true value of mu is 5, the average mu is 4.87. When the true mu is 6, the average mu is 6.42. The only exception is when mu is 1, the average mu is 2.18.
